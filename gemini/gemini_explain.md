# Gemini의 사과게임 AI 솔버 개발 프로젝트 설명서

## 1. 프로젝트 목표 및 초기 분석

사용자께서는 '사과게임'의 규칙(`game_rule.md`)을 기반으로, 정해진 로직이 아닌 머신러닝을 통해 게임을 해결하는 AI 개발을 요청하셨습니다. 핵심은 **"로직을 직접 짜지 않는"** 방식으로, AI가 스스로 최적의 플레이 방법을 학습하게 만드는 것이었습니다.

이 요구사항을 분석했을 때, 이 문제는 다음과 같은 특징을 가집니다.

- **순차적 의사결정 문제**: 매 순간 '어떤 사과를 없앨까?'라는 결정을 내려야 합니다.
- **결과의 지연성**: 현재의 결정이 당장의 보상뿐만 아니라, 미래에 더 좋은 수를 둘 수 있는지 없는지에 큰 영향을 미칩니다. (예: 당장 3개짜리 조합을 없애는 것보다, 길을 막는 2개짜리 조합을 없애는 것이 최종 점수에 더 유리할 수 있습니다.)
- **상태(State) 기반**: 모든 결정은 현재 '보드판의 상태'에 따라 내려집니다.

이러한 특징들은 **강화학습(Reinforcement Learning, RL)**이 가장 적합한 해결책임을 명확히 보여줍니다. 강화학습은 에이전트(Agent)가 환경(Environment)과 상호작용하며, 보상(Reward)을 최대화하는 방향으로 행동 정책(Policy)을 학습하는 머신러닝의 한 분야입니다.

## 2. 강화학습 접근법 설계

저는 이 문제를 해결하기 위해 **DQN(Deep Q-Network)** 알고리즘을 선택했습니다.

### 왜 DQN인가?

- **복잡한 상태 공간**: 게임 보드는 17x10 크기로, 가능한 모든 보드 상태의 수는 천문학적입니다. 전통적인 Q-러닝 방식(모든 상태-행동 쌍의 가치를 테이블에 저장)은 불가능합니다. DQN은 심층 신경망(Deep Neural Network)을 사용하여 Q-가치(행동의 가치)를 근사(approximation)하므로, 이런 복잡한 상태를 처리할 수 있습니다.
- **입증된 성능**: DQN은 아타리 게임과 같은 2D 격자 환경에서 뛰어난 성능을 보여준 바 있어, 사과게임에도 효과적일 것이라 판단했습니다.

### 사과게임의 강화학습 요소 정의

- **에이전트 (Agent)**: 우리가 개발하는 AI 플레이어 (`agent.py`의 `DQNAgent`).
- **환경 (Environment)**: 사과게임의 규칙을 시뮬레이션하는 가상 게임판 (`rl.py`의 `AppleGameEnv`).
- **상태 (State)**: 17x10 보드에 남은 사과들의 숫자와 위치. (Numpy 배열로 표현)
- **행동 (Action)**: 합이 10이 되는 사과 조합을 선택하는 모든 가능한 경우의 수. (좌표의 리스트로 표현, 예: `[(0,1), (5,8)]`)
- **보상 (Reward)**:
    - **긍정적 보상**: 행동으로 없앤 사과의 수.
    - **보너스 보상**: 게임을 완벽하게 클리어했을 때 주어지는 추가 점수 (+100).
    - **부정적 보상**: 규칙에 어긋나는 행동(합이 10이 아닌 조합 선택)을 시도했을 때의 페널티 (-10).

### 핵심 설계: 동적 행동 공간(Dynamic Action Space) 처리

사과게임의 가장 큰 기술적 난관은 '행동 공간'이 매 순간 변한다는 점입니다. 일반적인 DQN은 '상/하/좌/우'처럼 고정된 행동 공간을 갖지만, 사과게임에서는 현재 보드 상태에 따라 가능한 행동(합이 10이 되는 조합)의 종류와 개수가 계속 바뀝니다.

이 문제를 해결하기 위해 저는 다음과 같은 독특한 접근법을 사용했습니다.

> **AI가 '행동' 자체의 가치를 평가하는 대신, 그 행동으로 인해 도달하게 될 '다음 상태'의 가치를 평가하도록 설계했습니다.**

즉, `Q(상태, 행동)`을 계산할 때, 신경망은 `(상태, 행동)`을 직접 입력받는 것이 아니라, `상태`에서 `행동`을 취한 후의 **결과 상태**를 입력받아 그 상태의 가치 `V(결과 상태)`를 예측합니다. 이 예측값이 곧 `Q(상태, 행동)`의 근사치가 됩니다.

- **장점**: 행동 공간이 아무리 복잡하고 동적으로 변하더라도, 신경망은 일관되게 '상태'라는 고정된 형태의 입력만 처리하면 되므로 구조가 단순해지고 학습이 안정됩니다.
- **구현**: `agent.py`의 `select_action` 함수에서 이 로직을 확인할 수 있습니다. 가능한 모든 행동에 대해 가상의 다음 상태를 만들어보고, 신경망으로 각 상태의 가치를 평가한 뒤 가장 높은 가치를 가진 행동을 선택합니다.

## 3. 프로젝트 파일 구조 및 설명

프로젝트는 세 개의 핵심 파이썬 파일로 구성됩니다.

### 1. `rl.py` - 게임 환경

- **역할**: 사과게임의 규칙을 담고 있는 시뮬레이터입니다. 강화학습 에이전트가 상호작용할 '환경'을 제공합니다.
- **주요 구성**:
    - `AppleGameEnv` 클래스: OpenAI Gym의 환경 인터페이스와 유사한 구조로 설계되었습니다.
    - `reset()`: 게임 보드를 랜덤 숫자로 초기화하고, 초기 상태를 반환합니다.
    - `step(action)`: 에이전트로부터 행동을 입력받아 게임 상태를 업데이트하고, 보상과 종료 여부를 반환합니다.
    - `get_possible_actions()`: 현재 상태에서 가능한 모든 행동(합이 10이 되는 조합)을 탐색하여 리스트로 반환합니다. 이 함수는 에이전트가 무엇을 할 수 있는지 알려주는 중요한 역할을 합니다.

### 2. `agent.py` - DQN 에이전트

- **역할**: 게임을 플레이하고 학습하는 AI의 '뇌'입니다.
- **주요 구성**:
    - `DQN` 클래스 (신경망):
        - **CNN (Convolutional Neural Network)** 구조를 사용합니다. CNN은 이미지와 같은 격자 데이터에서 공간적 특징(인접한 사과의 관계 등)을 효과적으로 학습할 수 있어, 게임 보드 상태를 분석하는 데 매우 적합합니다.
        - 입력: `(1, 17, 10)` 크기의 보드 상태 텐서
        - 출력: 해당 상태의 가치를 나타내는 단일 숫자 (Q-value)
    - `ReplayMemory` 클래스:
        - 에이전트의 경험 `(상태, 행동, 보상, 다음 상태)`을 저장하는 메모리 버퍼입니다.
        - 무작위로 경험을 샘플링하여 학습에 사용함으로써, 데이터 간의 시간적 상관관계를 깨고 학습을 안정화시키는 중요한 역할을 합니다.
    - `DQNAgent` 클래스:
        - `policy_net` (정책망)과 `target_net` (타겟망)이라는 두 개의 신경망을 가집니다. 타겟망은 학습 목표(Target)를 안정적으로 유지하여 학습의 변동성을 줄여줍니다.
        - `select_action()`: 입실론-그리디(Epsilon-Greedy) 정책에 따라 행동을 선택합니다. 학습 초기에는 무작위 탐색(Exploration)을, 학습이 진행될수록 학습된 최적의 행동(Exploitation)을 선택합니다.
        - `_get_state_tensor_for_action()`: 위에서 설명한 '다음 상태의 가치 평가' 전략을 구현하는 핵심 헬퍼 함수입니다.

### 3. `train.py` - 학습 오케스트레이터

- **역할**: `AppleGameEnv`와 `DQNAgent`를 하나로 묶어 실제 학습 과정을 총괄하는 메인 스크립트입니다.
- **주요 로직**:
    - **하이퍼파라미터 설정**: `BATCH_SIZE`, `GAMMA` (미래 보상 할인율), `EPSILON` (탐험 확률) 등 학습에 필요한 주요 변수들을 정의합니다.
    - **학습 루프**: `NUM_EPISODES` 만큼 게임을 반복 실행합니다.
        - 각 스텝마다 환경과 에이전트가 상호작용하며 경험을 생성하고, 리플레이 메모리에 저장합니다.
        - `optimize_model()` 함수를 주기적으로 호출하여 메모리에서 배치(batch) 단위로 경험을 샘플링하고, 신경망의 가중치를 업데이트(학습)합니다. 이 함수는 DQN의 핵심인 손실(Loss) 계산 및 역전파(Backpropagation)를 수행합니다.
    - **진행 상황 모니터링**: 각 에피소드가 끝날 때마다 총점을 출력하여, 모델이 점차 개선되고 있는지를 확인할 수 있습니다.

## 4. 향후 개선 방안

이 프로젝트는 DQN을 이용한 강화학습 솔버의 기본적인 골격을 제공합니다. 성능을 더욱 향상시키기 위해 다음과 같은 시도를 해볼 수 있습니다.

1.  **모델 저장 및 평가**: 학습된 `policy_net`의 가중치를 파일로 저장하고, 별도의 평가 스크립트에서 이 모델을 불러와 탐욕적(greedy) 정책으로만 플레이하여 성능을 객관적으로 측정합니다.
2.  **하이퍼파라미터 튜닝**: `train.py`에 있는 학습률, 감마, 배치 사이즈 등의 하이퍼파라미터를 조정하여 더 빠르고 안정적인 학습을 유도합니다.
3.  **알고리즘 개선**: Double DQN, Dueling DQN 등 더 진보된 강화학습 알고리즘을 적용하여 성능을 높일 수 있습니다.
4.  **시각화**: `Pygame`과 같은 라이브러리를 사용하여, 학습된 에이전트가 실제로 게임을 플레이하는 모습을 시각적으로 보여주는 데모를 제작합니다.
